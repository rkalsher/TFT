{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2d978e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_formatters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The Google Research Authors.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"Custom formatting functions for Electricity dataset.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mDefines dataset specific column definitions and data transformations. Uses\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03mentity specific z-score normalization.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_formatters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_formatters'"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Custom formatting functions for Electricity dataset.\n",
    "\n",
    "Defines dataset specific column definitions and data transformations. Uses\n",
    "entity specific z-score normalization.\n",
    "\"\"\"\n",
    "import data_formatters\n",
    "import libs.utils as utils\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "GenericDataFormatter = data_formatters.base.GenericDataFormatter\n",
    "DataTypes = data_formatters.base.DataTypes\n",
    "InputTypes = data_formatters.base.InputTypes\n",
    "\n",
    "\n",
    "class ElectricityFormatter(GenericDataFormatter):\n",
    "  \"\"\"Defines and formats data for the electricity dataset.\n",
    "\n",
    "  Note that per-entity z-score normalization is used here, and is implemented\n",
    "  across functions.\n",
    "\n",
    "  Attributes:\n",
    "    column_definition: Defines input and data type of column used in the\n",
    "      experiment.\n",
    "    identifiers: Entity identifiers used in experiments.\n",
    "  \"\"\"\n",
    "\n",
    "  _column_definition = [\n",
    "      ('id', DataTypes.REAL_VALUED, InputTypes.ID),\n",
    "      ('hours_from_start', DataTypes.REAL_VALUED, InputTypes.TIME),\n",
    "      ('power_usage', DataTypes.REAL_VALUED, InputTypes.TARGET),\n",
    "      ('hour', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('day_of_week', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('hours_from_start', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('categorical_id', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "  ]\n",
    "\n",
    "def __init__(self):\n",
    "    \"\"\"Initialises formatter.\"\"\"\n",
    "\n",
    "    self.identifiers = None\n",
    "    self._real_scalers = None\n",
    "    self._cat_scalers = None\n",
    "    self._target_scaler = None\n",
    "    self._num_classes_per_cat_input = None\n",
    "    self._time_steps = self.get_fixed_params()['total_time_steps']\n",
    "\n",
    "def split_data(self, df, valid_boundary=1315, test_boundary=1339):\n",
    "    \"\"\"Splits data frame into training-validation-test data frames.\n",
    "\n",
    "    This also calibrates scaling object, and transforms data for each split.\n",
    "\n",
    "    Args:\n",
    "      df: Source data frame to split.\n",
    "      valid_boundary: Starting year for validation data\n",
    "      test_boundary: Starting year for test data\n",
    "\n",
    "    Returns:\n",
    "      Tuple of transformed (train, valid, test) data.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Formatting train-valid-test splits.')\n",
    "\n",
    "    index = df['days_from_start']\n",
    "    train = df.loc[index < valid_boundary]\n",
    "    valid = df.loc[(index >= valid_boundary - 7) & (index < test_boundary)]\n",
    "    test = df.loc[index >= test_boundary - 7]\n",
    "\n",
    "    self.set_scalers(train)\n",
    "\n",
    "    return (self.transform_inputs(data) for data in [train, valid, test])\n",
    "\n",
    "    def set_scalers(self, df):\n",
    "        \"\"\"Calibrates scalers using the data supplied.\n",
    "\n",
    "    Args:\n",
    "      df: Data to use to calibrate scalers.\n",
    "    \"\"\"\n",
    "    print('Setting scalers with training data...')\n",
    "\n",
    "    column_definitions = self.get_column_definition()\n",
    "    id_column = utils.get_single_col_by_input_type(InputTypes.ID,\n",
    "                                                   column_definitions)\n",
    "    target_column = utils.get_single_col_by_input_type(InputTypes.TARGET,\n",
    "                                                       column_definitions)\n",
    "\n",
    "    # Format real scalers\n",
    "    real_inputs = utils.extract_cols_from_data_type(\n",
    "        DataTypes.REAL_VALUED, column_definitions,\n",
    "        {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "    # Initialise scaler caches\n",
    "    self._real_scalers = {}\n",
    "    self._target_scaler = {}\n",
    "    identifiers = []\n",
    "    for identifier, sliced in df.groupby(id_column):\n",
    "\n",
    "        if len(sliced) >= self._time_steps:\n",
    "\n",
    "            data = sliced[real_inputs].values\n",
    "            targets = sliced[[target_column]].values\n",
    "            self._real_scalers[identifier] \\\n",
    "          = sklearn.preprocessing.StandardScaler().fit(data)\n",
    "\n",
    "        self._target_scaler[identifier] \\\n",
    "      = sklearn.preprocessing.StandardScaler().fit(targets)\n",
    "    identifiers.append(identifier)\n",
    "\n",
    "    # Format categorical scalers\n",
    "    categorical_inputs = utils.extract_cols_from_data_type(\n",
    "        DataTypes.CATEGORICAL, column_definitions,\n",
    "        {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "    categorical_scalers = {}\n",
    "    num_classes = []\n",
    "    for col in categorical_inputs:\n",
    "      # Set all to str so that we don't have mixed integer/string columns\n",
    "      srs = df[col].apply(str)\n",
    "    categorical_scalers[col] = sklearn.preprocessing.LabelEncoder().fit(\n",
    "          srs.values)\n",
    "    num_classes.append(srs.nunique())\n",
    "\n",
    "    # Set categorical scaler outputs\n",
    "    self._cat_scalers = categorical_scalers\n",
    "    self._num_classes_per_cat_input = num_classes\n",
    "\n",
    "    # Extract identifiers in case required\n",
    "    self.identifiers = identifiers\n",
    "\n",
    "def transform_inputs(self, df):\n",
    "    \"\"\"Performs feature transformations.\n",
    "\n",
    "    This includes both feature engineering, preprocessing and normalisation.\n",
    "\n",
    "    Args:\n",
    "      df: Data frame to transform.\n",
    "\n",
    "    Returns:\n",
    "      Transformed data frame.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if self._real_scalers is None and self._cat_scalers is None:\n",
    "        raise ValueError('Scalers have not been set!')\n",
    "\n",
    "    # Extract relevant columns\n",
    "    column_definitions = self.get_column_definition()\n",
    "    id_col = utils.get_single_col_by_input_type(InputTypes.ID,\n",
    "                                                column_definitions)\n",
    "    real_inputs = utils.extract_cols_from_data_type(\n",
    "        DataTypes.REAL_VALUED, column_definitions,\n",
    "        {InputTypes.ID, InputTypes.TIME})\n",
    "    categorical_inputs = utils.extract_cols_from_data_type(\n",
    "        DataTypes.CATEGORICAL, column_definitions,\n",
    "        {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "    # Transform real inputs per entity\n",
    "    df_list = []\n",
    "    for identifier, sliced in df.groupby(id_col):\n",
    "\n",
    "      # Filter out any trajectories that are too short\n",
    "      if len(sliced) >= self._time_steps:\n",
    "        sliced_copy = sliced.copy()\n",
    "        sliced_copy[real_inputs] = self._real_scalers[identifier].transform(\n",
    "            sliced_copy[real_inputs].values)\n",
    "        df_list.append(sliced_copy)\n",
    "\n",
    "    output = pd.concat(df_list, axis=0)\n",
    "\n",
    "    # Format categorical inputs\n",
    "    for col in categorical_inputs:\n",
    "        string_df = df[col].apply(str)\n",
    "        output[col] = self._cat_scalers[col].transform(string_df)\n",
    "\n",
    "    return output\n",
    "\n",
    "    def format_predictions(self, predictions):\n",
    "        \"\"\"Reverts any normalisation to give predictions in original scale.\n",
    "\n",
    "    Args:\n",
    "      predictions: Dataframe of model predictions.\n",
    "\n",
    "    Returns:\n",
    "      Data frame of unnormalised predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    if self._target_scaler is None:\n",
    "        raise ValueError('Scalers have not been set!')\n",
    "\n",
    "    column_names = predictions.columns\n",
    "\n",
    "    df_list = []\n",
    "    for identifier, sliced in predictions.groupby('identifier'):\n",
    "        sliced_copy = sliced.copy()\n",
    "        target_scaler = self._target_scaler[identifier]\n",
    "\n",
    "    for col in column_names:\n",
    "        if col not in {'forecast_time', 'identifier'}:\n",
    "            sliced_copy[col] = target_scaler.inverse_transform(sliced_copy[col])\n",
    "            df_list.append(sliced_copy)\n",
    "\n",
    "    output = pd.concat(df_list, axis=0)\n",
    "\n",
    "    return output\n",
    "\n",
    "  # Default params\n",
    "def get_fixed_params(self):\n",
    "    \"\"\"Returns fixed model parameters for experiments.\"\"\"\n",
    "\n",
    "    fixed_params = {\n",
    "        'total_time_steps': 8 * 24,\n",
    "        'num_encoder_steps': 7 * 24,\n",
    "        'num_epochs': 100,\n",
    "        'early_stopping_patience': 5,\n",
    "        'multiprocessing_workers': 5\n",
    "    }\n",
    "\n",
    "    return fixed_params\n",
    "\n",
    "    def get_default_model_params(self):\n",
    "         \"\"\"Returns default optimised model parameters.\"\"\"\n",
    "\n",
    "    model_params = {\n",
    "        'dropout_rate': 0.1,\n",
    "        'hidden_layer_size': 160,\n",
    "        'learning_rate': 0.001,\n",
    "        'minibatch_size': 64,\n",
    "        'max_gradient_norm': 0.01,\n",
    "        'num_heads': 4,\n",
    "        'stack_size': 1\n",
    "    }\n",
    "\n",
    "    return model_params\n",
    "\n",
    "    def get_num_samples_for_calibration(self):\n",
    "        \"\"\"Gets the default number of training and validation samples.\n",
    "\n",
    "    Use to sub-sample the data for network calibration and a value of -1 uses\n",
    "    all available samples.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (training samples, validation samples)\n",
    "    \"\"\"\n",
    "    return 450000, 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c2f6d2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_formatters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_formatters\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_formatters'"
     ]
    }
   ],
   "source": [
    "import data_formatters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e06afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
